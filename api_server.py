"""FastAPI ì„œë²„: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ + ONNX ì¶”ë¡ """

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, Response
from PIL import Image
import io
import numpy as np
import onnxruntime as ort
import albumentations as A
from albumentations.pytorch import ToTensorV2
import yaml
from rembg import remove, new_session
import os
import time

# preprocess.pyì—ì„œ crop í•¨ìˆ˜ import
from preprocess import crop
from inference import class_names

app = FastAPI(title="Car Model Prediction API")

# CORS ì„¤ì •
import os

allowed_origins_env = os.getenv(
    "ALLOWED_ORIGINS",
    "https://carvisionwebsite.up.railway.app,http://localhost:5173,http://localhost:5174,http://localhost:5175,http://localhost:5176,http://localhost",
)

ALLOWED_ORIGINS = [
    origin.strip() for origin in allowed_origins_env.split(",") if origin.strip()
]

print("âœ… Loaded ALLOWED_ORIGINS:", ALLOWED_ORIGINS)

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
    allow_credentials=True,
)

print("ALLOWED_ORIGINS =", ALLOWED_ORIGINS)


# config ë¡œë“œ
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

# Transform (inference.pyì™€ ë™ì¼)
transform = A.Compose(
    [
        A.Resize(384, 384),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
    ]
)

# ONNX ì„¸ì…˜ (ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
print("=" * 70)
print("ONNX ëª¨ë¸ ë¡œë”© ì¤‘...")
onnx_session = ort.InferenceSession("Bestefficientnet.onnx")
print("âœ“ ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(
    f"ì…ë ¥: {onnx_session.get_inputs()[0].name}, shape: {onnx_session.get_inputs()[0].shape}"
)
print(
    f"ì¶œë ¥: {onnx_session.get_outputs()[0].name}, shape: {onnx_session.get_outputs()[0].shape}"
)
print("=" * 70 + "\n")

# Rembg ì„¸ì…˜ (ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
print("=" * 70)
print("ğŸ”¥ Rembg ì„¸ì…˜ ë¡œë”© ì¤‘ (cold start)...")
rembg_session = new_session("u2net")
print("âœ… Rembg ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ (ë©”ëª¨ë¦¬ì— ìºì‹œë¨)")
print("=" * 70 + "\n")


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Car Model Prediction API",
        "endpoints": {
            "GET /": "API ì •ë³´",
            "GET /health": "í—¬ìŠ¤ ì²´í¬",
            "POST /predict": "ì´ë¯¸ì§€ ì˜ˆì¸¡ (multipart/form-data, field: image)",
        },
    }


@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "model": "Bestefficientnet.onnx",
        "num_classes": len(class_names),
    }


@app.get("/default-image")
async def get_default_image():
    """ê¸°ë³¸ ì´ë¯¸ì§€(DefaultTucson.jpg) ì œê³µ"""
    # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ DefaultTucson.jpg ì°¾ê¸°
    image_path = "DefaultTucson.jpg"

    if not os.path.exists(image_path):
        # car-finder/public í´ë”ì—ì„œ ì°¾ê¸°
        image_path = os.path.join("car-finder", "public", "DefaultTucson.jpg")

    if not os.path.exists(image_path):
        raise HTTPException(
            status_code=404, detail="DefaultTucson.jpg íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )

    # FileResponse ì‚¬ìš© (CORS ë¯¸ë“¤ì›¨ì–´ê°€ ìë™ìœ¼ë¡œ í—¤ë” ì¶”ê°€)
    return FileResponse(image_path, media_type="image/jpeg")


@app.post("/predict")
async def predict(image: UploadFile = File(...)):
    """
    ì´ë¯¸ì§€ ì—…ë¡œë“œ â†’ ì „ì²˜ë¦¬ (ë°°ê²½ ì œê±° + í¬ë¡­) â†’ ONNX ì¶”ë¡  â†’ Top-5 ê²°ê³¼ ë°˜í™˜

    Parameters:
    - image: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼

    Returns:
    - predictions: Top-5 ì˜ˆì¸¡ ê²°ê³¼
    """
    try:
        tstart = time.time()
        # 1. ì´ë¯¸ì§€ ì½ê¸°
        print("1. ì´ë¯¸ì§€ ìˆ˜ì‹  ì¤‘...")
        image_bytes = await image.read()
        pil_image = Image.open(io.BytesIO(image_bytes))
        print(f"   âœ“ ì´ë¯¸ì§€ í¬ê¸°: {pil_image.size}, ëª¨ë“œ: {pil_image.mode}")
        t1 = time.time()
        print(f"   âœ“ ì´ë¯¸ì§€ ìˆ˜ì‹  ì™„ë£Œ: {t1 - tstart:.2f}ì´ˆ")

        # 2. ë°°ê²½ ì œê±° (ìºì‹œëœ ì„¸ì…˜ ì‚¬ìš©)
        print("2. ë°°ê²½ ì œê±° ì¤‘ (ìºì‹œëœ ì„¸ì…˜ ì‚¬ìš©)...")
        removed_bg = remove(pil_image, session=rembg_session)
        if isinstance(removed_bg, bytes):
            removed_bg = Image.open(io.BytesIO(removed_bg))
        t2 = time.time()
        print(f"   âœ“ ë°°ê²½ ì œê±° ì™„ë£Œ: {removed_bg.size}, {removed_bg.mode}")
        print(f"   âœ“ ë°°ê²½ ì œê±° ì†Œìš” ì‹œê°„: {t2 - t1:.2f}ì´ˆ (ìºì‹œëœ ì„¸ì…˜)")

        # 3. ìë™ í¬ë¡­ (ì—¬ë°± ì œê±°)
        print("3. ìë™ í¬ë¡­ ì¤‘...")
        cropped = crop(removed_bg)
        t3 = time.time()
        print(f"   âœ“ í¬ë¡­ ì™„ë£Œ: {cropped.size}")
        print(f"   âœ“ í¬ë¡­ ì†Œìš” ì‹œê°„: {t3 - t2:.2f}ì´ˆ")

        # 4. RGB ë³€í™˜
        print("4. RGB ë³€í™˜ ì¤‘...")
        if cropped.mode == "RGBA":
            # íˆ¬ëª… ë°°ê²½ì„ í°ìƒ‰ìœ¼ë¡œ
            bg = Image.new("RGB", cropped.size, (255, 255, 255))
            bg.paste(cropped, mask=cropped.split()[3])
            cropped = bg
        elif cropped.mode != "RGB":
            cropped = cropped.convert("RGB")
        t4 = time.time()
        print(f"   âœ“ RGB ë³€í™˜ ì™„ë£Œ: {cropped.mode}")
        print(f"   âœ“ RGB ë³€í™˜ ì†Œìš” ì‹œê°„: {t4 - t3:.2f}ì´ˆ")

        # 5. Transform ì ìš©
        print("5. Transform ì ìš© ì¤‘...")
        image_np = np.array(cropped)
        transformed = transform(image=image_np)
        image_tensor = transformed["image"]

        # numpyë¡œ ë³€í™˜ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        image_tensor_np = image_tensor.numpy()
        image_tensor_np = np.expand_dims(image_tensor_np, axis=0).astype(np.float32)
        t5 = time.time()
        print(f"   âœ“ Tensor shape: {image_tensor_np.shape}")
        print(f"   âœ“ Transform ì†Œìš” ì‹œê°„: {t5 - t4:.2f}ì´ˆ")

        # 6. ONNX ì¶”ë¡ 
        print("6. ONNX ì¶”ë¡  ì¤‘...")
        input_name = onnx_session.get_inputs()[0].name
        output_name = onnx_session.get_outputs()[0].name

        outputs = onnx_session.run([output_name], {input_name: image_tensor_np})
        logits = outputs[0][0]
        t6 = time.time()
        print(f"   âœ“ Logits shape: {logits.shape}")
        print(f"   âœ“ ONNX ì¶”ë¡  ì†Œìš” ì‹œê°„: {t6 - t5:.2f}ì´ˆ")

        # 7. Softmax ì ìš©
        print("7. Softmax ì ìš© ì¤‘...")
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits)


        # 8. Top-5 ì¶”ì¶œ
        print("8. Top-5 ì¶”ì¶œ ì¤‘...")
        top5_indices = np.argsort(probs)[::-1][:5]
        predictions = []

        for rank, idx in enumerate(top5_indices, 1):
            predictions.append(
                {
                    "rank": rank,
                    "model": class_names[idx],
                    "confidence": float(probs[idx] * 100),
                }
            )
            print(f"   {rank}. {class_names[idx]}: {probs[idx]*100:.2f}%")
        tlast = time.time()
        print(f"   âœ“ Top-5 ì¶”ì¶œ ì†Œìš” ì‹œê°„: {tlast - t6:.2f}ì´ˆ")
        print(f"{'='*70}\n")

        return {
            "success": True,
            "top_model": predictions[0]["model"],
            "top_confidence": predictions[0]["confidence"],
            "predictions": predictions,
        }

    except Exception as e:
        print(f"\nâœ— ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    print("\n" + "=" * 70)
    print("FastAPI ì„œë²„ ì‹œì‘")
    print("=" * 70)
    print("URL: http://localhost:8000")
    print("Docs: http://localhost:8000/docs")
    print("=" * 70 + "\n")

    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
